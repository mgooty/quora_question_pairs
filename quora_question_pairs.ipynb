{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/dev_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['question1'].notnull()]\n",
    "train_df = train_df[train_df['question2'].notnull()]\n",
    "\n",
    "question1_sentences = []\n",
    "question2_sentences = []\n",
    "labels = []\n",
    "for index, row in train_df.iterrows():\n",
    "    question1_sentences.append(text_to_wordlist(row['question1']))\n",
    "    question2_sentences.append(text_to_wordlist(row['question2']))\n",
    "    labels.append(int(row['is_duplicate']))\n",
    "\n",
    "assert (len(question1_sentences) == len(question2_sentences)), \"Num of q1 and q2 are not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wordsList = []\n",
    "\n",
    "for row in question1_sentences:\n",
    "    wordsList.extend(row.split())\n",
    "    \n",
    "for row in question2_sentences:\n",
    "    wordsList.extend(row.split())\n",
    "        \n",
    "counts = Counter(wordsList)\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86005"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "question1_int = []\n",
    "question2_int = []\n",
    "labels_int = []\n",
    "\n",
    "for index, row in enumerate(question1_sentences):\n",
    "    question1 = []\n",
    "    question2 = []\n",
    "    \n",
    "    question1_words = question1_sentences[index].split()\n",
    "    question2_words = question2_sentences[index].split()\n",
    "    \n",
    "    if (len(question1_words) > 3) and (len(question2_words) > 3):\n",
    "        labels_int.append(labels[index])\n",
    "        for word in question1_words:\n",
    "            question1.append(vocab_to_int.get(word))\n",
    "\n",
    "        question1_int.append(question1)\n",
    "\n",
    "        for word in question2_words:\n",
    "            question2.append(vocab_to_int.get(word))\n",
    "\n",
    "        question2_int.append(question2)\n",
    "        \n",
    "assert (len(question1_int) == len(question2_int)), \"Num of q1 and q2 ints are not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "question1_features = np.zeros((len(question1_int), seq_len), dtype=int)\n",
    "for i, row in enumerate(question1_int):\n",
    "    if len(row) == 0:\n",
    "        print('something is wrong here - {}', i)\n",
    "        \n",
    "    question1_features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "question2_features = np.zeros((len(question2_int), seq_len), dtype=int)\n",
    "for i, row in enumerate(question2_int):\n",
    "    question2_features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "assert (len(question1_features) == len(question2_features)), \"Num of q1 and q2 features are not equal\"\n",
    "assert (len(question1_features) == len(labels_int)), \"Num of questions and labels are not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(vocab_to_int)+1, 300), dtype=float)\n",
    "\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in word_embedding_model.vocab:\n",
    "        embedding_matrix[i] = word_embedding_model.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "labels = np.asarray(labels_int)\n",
    "train_idx, val_idx = next(ss.split(question1_features, labels))\n",
    "\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "question1_train, question2_train, label_train = question1_features[train_idx], question2_features[train_idx], labels[train_idx]\n",
    "question1_val, question2_val, label_val = question1_features[val_idx], question2_features[val_idx], labels[val_idx]\n",
    "question1_test, question2_test, label_test = question1_features[test_idx], question2_features[test_idx], labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39890"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 64\n",
    "lstm_layers = 1\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86006, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, shape=embedding_matrix.shape, name='embedding_placeholder')\n",
    "    with tf.variable_scope('siamese_network') as scope:\n",
    "        labels = tf.placeholder(tf.int32, [batch_size, None], name='labels')\n",
    "        keep_prob = tf.placeholder(tf.float32, name='question1_keep_prob')\n",
    "        \n",
    "        with tf.name_scope('question1') as question1_scope:\n",
    "            question1_inputs = tf.placeholder(tf.int32, [batch_size, seq_len], name='question1_inputs')\n",
    "            \n",
    "            question1_embedding = tf.get_variable(name='embedding', initializer=embedding_placeholder, trainable=False)\n",
    "            question1_embed = tf.nn.embedding_lookup(question1_embedding, question1_inputs)\n",
    "\n",
    "            # question1_lstm = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_size, forget_bias=0.0)\n",
    "            question1_lstm = tf.contrib.rnn.LSTMCell(lstm_size, tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32))\n",
    "            question1_drop = tf.contrib.rnn.DropoutWrapper(question1_lstm, output_keep_prob=keep_prob)\n",
    "            question1_multi_lstm = tf.contrib.rnn.MultiRNNCell([question1_drop] * lstm_layers)\n",
    "\n",
    "            q1_initial_state = question1_multi_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "            question1_outputs, question1_final_state = tf.nn.dynamic_rnn(question1_multi_lstm, question1_embed, initial_state=q1_initial_state)\n",
    "            question1_predictions = tf.contrib.layers.fully_connected(question1_outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "\n",
    "        scope.reuse_variables()\n",
    "        \n",
    "        with tf.name_scope('question2') as question2_scope:\n",
    "            question2_inputs = tf.placeholder(tf.int32, [batch_size, seq_len], name='question2_inputs')\n",
    "\n",
    "            question2_embedding = question1_embedding\n",
    "            question2_embed = tf.nn.embedding_lookup(question2_embedding, question2_inputs)\n",
    "\n",
    "            # question2_lstm = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_size, forget_bias=0.0)\n",
    "            question2_lstm = tf.contrib.rnn.LSTMCell(lstm_size, tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32))\n",
    "            question2_drop = tf.contrib.rnn.DropoutWrapper(question2_lstm, output_keep_prob=keep_prob)\n",
    "            question2_multi_lstm = tf.contrib.rnn.MultiRNNCell([question2_drop] * lstm_layers)\n",
    "\n",
    "            q2_initial_state = question2_multi_lstm.zero_state(batch_size, tf.float32)\n",
    "            \n",
    "            question2_outputs, question2_final_state = tf.nn.dynamic_rnn(question2_multi_lstm, question2_embed, initial_state=q2_initial_state)\n",
    "            question2_predictions = tf.contrib.layers.fully_connected(question2_outputs[:, -1], 1, activation_fn=tf.sigmoid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    diff = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(question1_outputs[:, -1, :], question2_outputs[:, -1, :])), reduction_indices=1))\n",
    "\n",
    "    margin = tf.constant(1.) \n",
    "    labels = tf.to_float(labels)\n",
    "    match_loss = tf.expand_dims(tf.square(diff, 'match_term'), 0)\n",
    "    mismatch_loss = tf.expand_dims(tf.maximum(0., tf.subtract(margin, tf.square(diff)), 'mismatch_term'), 0)\n",
    "\n",
    "    loss = tf.add(tf.matmul(labels, match_loss), tf.matmul((1 - labels), mismatch_loss), 'loss_add')\n",
    "    distance = tf.reduce_mean(loss)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x1_input, x2_input, y, batch_size=100):\n",
    "    n_batches = len(x1_input)//batch_size\n",
    "    x1, x2, y = x1_input[:n_batches*batch_size], x2_input[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x1), batch_size):\n",
    "        yield x1[ii:ii+batch_size], x2[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_batches(x1_input, x2_input, test_ids, batch_size=100):\n",
    "    n_batches = len(x1_input)//batch_size\n",
    "    x1, x2= x1_input[:n_batches*batch_size], x2_input[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x1), batch_size):\n",
    "        yield x1[ii:ii+batch_size], x2[ii:ii+batch_size], test_ids[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Validation Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(diff), tf.float32), labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={embedding_placeholder: embedding_matrix})\n",
    "    \n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        # q1_int_state = sess.run(q1_initial_state)\n",
    "        # q2_int_state = sess.run(q2_initial_state)\n",
    "        # summary_writer = tf.summary.FileWriter('/Users/mithun/projects/kaggle/quora_question_pairs/logs', sess.graph)\n",
    "        # summary_writer.add_graph(sess.graph)\n",
    "        \n",
    "        iteration = 0\n",
    "        for ii, (x1, x2, y) in enumerate(get_batches(question1_train, question2_train, label_train, batch_size), 1):\n",
    "            feed = {question1_inputs: x1,\n",
    "                    question2_inputs: x2,\n",
    "                    labels: y[:, None],\n",
    "                    keep_prob: 0.9,\n",
    "                    # q1_initial_state: q1_int_state,\n",
    "                    # q2_initial_state: q2_int_state\n",
    "                   }\n",
    "            loss1, q2_out, q1_int_state, q2_int_state = sess.run([distance, question2_outputs, question1_final_state, question2_final_state], feed_dict=feed)\n",
    "            \n",
    "            if iteration%25==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss1))\n",
    "                \n",
    "            iteration +=1\n",
    "            \n",
    "        #if iteration%50==0:\n",
    "        val_acc = []\n",
    "        # q1_val_int_state = sess.run(q1_initial_state)\n",
    "        # q2_val_int_state = sess.run(q2_initial_state)\n",
    "        for x1, x2, y in get_batches(question1_val, question2_val, label_val, batch_size):\n",
    "            feed = {question1_inputs: x1,\n",
    "                    question2_inputs: x2,\n",
    "                    labels: y[:, None],\n",
    "                    keep_prob: 1,\n",
    "                    # q1_initial_state: q1_val_int_state,\n",
    "                    # q2_initial_state: q2_val_int_state\n",
    "                   }\n",
    "            dist1, correct_pred1, batch_acc, q1_val_int_state, q2_val_int_state = sess.run([diff, correct_pred, accuracy, question1_final_state, question2_final_state], feed_dict=feed)\n",
    "            val_acc.append(batch_acc)\n",
    "        print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            \n",
    "    # saver.save(sess, \"checkpoints/quora_pairs.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/dev_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['question1'].notnull()]\n",
    "test_df = test_df[test_df['question2'].notnull()]\n",
    "\n",
    "test_question1_int = []\n",
    "test_question2_int = []\n",
    "test_ids = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    test_question1_words = text_to_wordlist(row['question1'])\n",
    "    test_question2_words = text_to_wordlist(row['question2'])\n",
    "    question1 = []\n",
    "    question2 = []\n",
    "    \n",
    "    if (len(test_question1_words) > 3) and (len(test_question2_words) > 3):\n",
    "        for word in test_question1_words:\n",
    "            question1.append(vocab_to_int.get(word, 0))\n",
    "\n",
    "        test_question1_int.append(question1)\n",
    "\n",
    "        for word in test_question2_words:\n",
    "            question2.append(vocab_to_int.get(word, 0))\n",
    "\n",
    "        test_question2_int.append(question2)\n",
    "        test_ids.append(row['test_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_question1_features = np.zeros((len(test_question1_int), seq_len), dtype=int)\n",
    "for i, row in enumerate(test_question1_int):\n",
    "    test_question1_features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "test_question2_features = np.zeros((len(test_question2_int), seq_len), dtype=int)\n",
    "for i, row in enumerate(test_question2_int):\n",
    "    test_question2_features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    \n",
    "assert (len(test_question1_features) == len(test_question2_features)), \"Num of q1 and q2 features are not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={embedding_placeholder: embedding_matrix})\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    #test_state = sess.run(initial_state)\n",
    "    \n",
    "    for ii, (x1, x2, batch_test_ids) in enumerate(get_test_batches(test_question1_features, test_question2_features, test_ids, batch_size), 1):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_acc = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={embedding_placeholder: embedding_matrix})\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    #test_state = sess.run(initial_state)\n",
    "    q1_test_int_state = sess.run(q1_initial_state)\n",
    "    q2_test_int_state = sess.run(q2_initial_state)\n",
    "    \n",
    "    for ii, (x1, x2, batch_test_ids) in enumerate(get_test_batches(test_question1_features, test_question2_features, test_ids, batch_size), 1):\n",
    "        feed = {question1_inputs: x1,\n",
    "                question2_inputs: x2,\n",
    "                keep_prob: 1,\n",
    "                q1_initial_state: q1_test_int_state,\n",
    "                q2_initial_state: q2_test_int_state\n",
    "               }\n",
    "        test_state, distance1, q1_test_int_state, q1_test_int_state = sess.run([question1_final_state, distance, question1_final_state, question2_final_state], feed_dict=feed)\n",
    "        \n",
    "        prediction = (distance1 < 0.5).astype(int)\n",
    "        submission = pd.DataFrame({'test_id':batch_test_ids, 'is_duplicate':prediction.ravel(), 'q1_pred': q1_pred.ravel(), 'q2_pred': q2_pred.ravel()}, columns=[\"test_id\", \"is_duplicate\", \"q1_pred\", \"q2_pred\"])\n",
    "        submission.to_csv('submission.csv', mode='a', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
